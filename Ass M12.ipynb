{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOVnuZ6Py57CX3pyhEr2n40"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Theoretical Answers:-**"],"metadata":{"id":"Le2vF2GjgAJl"}},{"cell_type":"markdown","source":["# Ans 1:-\n","\n","Unsupervised learning is a type of machine learning where the algorithm learns patterns from unlabeled data. Unlike supervised learning, where the model is trained on a dataset that includes both input features and known outputs (labels), unsupervised learning works only with input features and tries to discover the structure or patterns within the data on its own.\n","\n"],"metadata":{"id":"odXYg4wTgALY"}},{"cell_type":"markdown","source":["# **Ans 2:-**\n","\n","**How It Works:-**\n","\n","1. Initialize Centroids\n","\n","2. Assign Data Points to Nearest Centroid\n","\n","3. Update Centroids\n","\n","4. Repeat\n","\n"," **Example**\n","\n","Imagine you have dots scattered on a 2D plane (X and Y axes), and you want to group them into 3 clusters:\n","\n","Choose 3 random points as initial centroids.\n","\n","Group nearby dots around each centroid.\n","\n","Recalculate the center (mean) of each group.\n","\n","Repeat until the groups become stable."],"metadata":{"id":"R-XYclnwgAPW"}},{"cell_type":"markdown","source":["# **Ans 3:-**\n","\n","A dendrogram is a tree-like diagram that shows how data points (or clusters) are merged step by step in hierarchical clustering. It visually represents the process of building the hierarchy of clusters.\n","\n","Think of it as a family tree — but for data points.\n","\n","**Where It's Used:**\n","\n","Customer segmentation\n","\n","Gene analysis in biology\n","\n","Document or image clustering"],"metadata":{"id":"rOFolwPCgARL"}},{"cell_type":"markdown","source":["# **Ans 4:-**\n","\n"," **1. Need to specify number of clusters**\n","\n","K-Means Clustering:\n","\n","➤ Yes, you must specify the number of clusters K before the algorithm starts.\n","\n","➤ Example: If you set K = 3, K-Means will divide your data into exactly 3 clusters.\n","\n","Hierarchical Clustering:\n","\n","➤ No, you don’t need to choose the number of clusters at the start.\n","\n","➤ It builds a full tree (dendrogram) showing how all points are merged step by step.\n","\n","➤ You can then cut the tree at the level you want to get the desired number of clusters.\n","\n","**2. Clustering Approach**\n","\n","K-Means Clustering:\n","\n","➤ It uses a partitioning approach.\n","\n","➤ It divides the dataset into K distinct, non-overlapping groups based on the nearest centroid.\n","\n","➤ All clusters are formed at once, and the algorithm tries to optimize cluster centers.\n","\n","Hierarchical Clustering:\n","\n","➤ It uses a hierarchical approach.\n","\n","➤ It either starts with:\n","\n","Agglomerative (bottom-up): Each data point starts in its own cluster, and pairs are merged step by step.\n","\n","Divisive (top-down): Starts with one big cluster and splits it recursively. ➤ It forms a tree-like structure of clusters."],"metadata":{"id":"38tus6FEgAUx"}},{"cell_type":"markdown","source":["# **Ans 5:-**\n","\n","✅ 1. No need to specify number of clusters\n","\n","✅ 2. Can find clusters of arbitrary shapes\n","\n","✅ 3. Better at handling noise and outliers\n","\n","✅ 4. No need to reassign points repeatedly"],"metadata":{"id":"ildyfEotgAXG"}},{"cell_type":"markdown","source":["# **Ans 6:-**\n","\n","When to Use Silhouette Score:-\n","\n"," 1. Evaluate clustering performance\n","\n"," 2. Choose the optimal number of clusters (K)\n","\n"," 3. Compare different clustering algorithms"],"metadata":{"id":"PrUdwpS8gAag"}},{"cell_type":"markdown","source":["# **Ans 7:-**\n","\n","Limitations of Hierarchical Clustering :-\n","\n"," 1. Not scalable for large datasets\n","\n"," 2. Irreversible merging/splitting\n","\n"," 3. Sensitive to noise and outliers\n","\n"," 4. Choice of linkage and distance matters\n","\n"," 5. Hard to decide number of clusters\n","\n"," 6. Can't handle non-hierarchical structure well"],"metadata":{"id":"hetjChh7gAcS"}},{"cell_type":"markdown","source":["# **Ans 8:-**\n","\n","**Distance Matters a Lot in K-Means**\n","\n","K-Means groups data based on distances (usually Euclidean distance) between points and centroids.\n","\n","If one feature (column) has much larger values than others, it will dominate the distance calculation, and your clusters can become biased or meaningless."],"metadata":{"id":"YqMWehoSgAf6"}},{"cell_type":"markdown","source":["# **Ans 9:-**\n","\n","**Core Idea of DBSCAN**\n","\n","DBSCAN groups data based on density — how close data points are to each other. It uses two key parameters:\n","\n","eps – the maximum distance to consider two points as \"neighbors\"\n","\n","minPts – the minimum number of points required to form a dense region (cluster)"],"metadata":{"id":"az5NOqyBgAiv"}},{"cell_type":"markdown","source":["# **Ans 10:-**\n","\n","In K-Means, inertia is a measure of how well the data points are clustered around their respective centroids.\n","\n","Inertia = The sum of squared distances between each data point and its assigned cluster centroid."],"metadata":{"id":"ZQLP7nwEgAob"}},{"cell_type":"markdown","source":["# Ans 11:-\n","\n","It is a graphical method to determine the best value for K by plotting the inertia (within-cluster sum of squared distances) for different values of K."],"metadata":{"id":"92lNAlgEf-3m"}},{"cell_type":"markdown","source":["# Ans 12:-\n","\n","In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), density refers to how closely packed the data points are in a region.\n","\n","➤ A region is dense if there are enough points close together.\n","➤ DBSCAN uses this to decide where clusters start and end."],"metadata":{"id":"6iNvVC4plAUm"}},{"cell_type":"markdown","source":["# Ans 13:-\n","\n","Yes, hierarchical clustering can be used on categorical data, but with a few special considerations."],"metadata":{"id":"d9PU1BctlACs"}},{"cell_type":"markdown","source":["# Ans 14:-\n","\n"," **Negative Silhouette Score **\n","\n","A Silhouette Score measures how well a data point fits into its assigned cluster vs. how close it is to the nearest neighboring cluster."],"metadata":{"id":"cnHc-o0Zk_2h"}},{"cell_type":"markdown","source":["# Ans 15:-\n","\n","Linkage criteria determine how the distance between two clusters is calculated when merging them in hierarchical clustering."],"metadata":{"id":"HOxe5Rxqk_qH"}},{"cell_type":"markdown","source":["# Ans 16:-\n","\n","**Why K-Means Struggles:**\n","\n","**1. Assumes Equal Cluster Sizes**\n","\n","K-Means assumes all clusters are roughly the same size and shape (i.e., spherical and equally sized).\n","\n","If your data has small and large clusters, K-Means may:\n","\n","Split large clusters into multiple pieces\n","\n","Merge small clusters into one big group\n","\n","➤ Example:\n","A small dense cluster next to a large loose cluster may get ignored or absorbed by the large one.\n","\n","**2. Assumes Uniform Densities**\n","\n","K-Means doesn’t consider how packed or spread out the data points are (i.e., density).\n","\n","It treats all points equally based only on distance to centroids.\n","\n","So it can misclassify points in sparse areas or pull centroids away from dense centers.\n","\n","**3. Sensitive to Outliers**\n","\n","Outliers (especially in sparse clusters) can drag the centroid, causing poor boundaries."],"metadata":{"id":"l-9KkWPAk_dB"}},{"cell_type":"markdown","source":["# Ans 17:-\n","\n","**DBSCAN has two main parameters:**\n","\n","eps (epsilon) – Neighborhood Radius\n","\n","minPts (minimum points) – Minimum number of points to form a dense region\n"],"metadata":{"id":"kyeDDH3dk_QX"}},{"cell_type":"markdown","source":["# Ans 18:-\n","\n","**Problem with Standard K-Means Initialization:**\n","\n","K-Means randomly selects k points as initial centroids.\n","\n","This randomness can lead to\n","\n","Poor clustering results\n","\n","Slow convergence\n","\n","Different results on different runs\n","\n","Clusters getting stuck in local minima\n"],"metadata":{"id":"vqr_T81Uk_Dx"}},{"cell_type":"markdown","source":["# Ans 19:-\n","\n","Agglomerative clustering is a bottom-up hierarchical clustering method where:\n","\n","Each data point starts as its own cluster\n","\n","Clusters are merged step-by-step based on similarity\n","\n","The process continues until all points belong to a single cluster"],"metadata":{"id":"n8sHDqeDk-3H"}},{"cell_type":"markdown","source":["# Ans 20:-\n","\n","Why Silhouette Score is Better:\n","1. Inertia always decreases with more clusters\n","\n","2. Silhouette Score balances compactness & separation\n","\n","3. Silhouette helps detect bad clustering\n","\n","4. Inertia is scale-dependent"],"metadata":{"id":"yAvYIZj6k-M8"}}]}